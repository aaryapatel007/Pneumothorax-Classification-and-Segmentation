{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install albumentations > /dev/null\n!pip install -U segmentation-models\n#!pip install -U efficientnet\nimport numpy as np\nimport pandas as pd\nimport gc\nimport keras\nimport itertools\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n\nfrom skimage.transform import resize\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.losses import binary_crossentropy\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import  ModelCheckpoint\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.losses import binary_crossentropy\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\nfrom keras.applications.xception import Xception\nfrom keras.layers import multiply\n\n\nfrom keras import optimizers\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.regularizers import l2\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute, GlobalMaxPooling2D\nfrom keras.optimizers import SGD,Adam\nfrom keras.models import Model\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.applications import NASNetMobile,MobileNetV2,densenet,resnet50,xception,VGG16\n\nfrom keras_applications.resnext import ResNeXt50\n\nimport glob\nimport shutil\nimport os\nimport random\nfrom PIL import Image\nimport cv2\nfrom random import shuffle\nseed = 10\nnp.random.seed(seed)\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\n!pip install image-classifiers==1.0.0\nfrom classification_models.keras import Classifiers\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Dataframe containing file_path, mask percentage and corresponding label(pneumothorax or no pneumothorax)"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_mask_fn = glob.glob('/kaggle/input/siimacr-pneumothorax-segmentation-data-512/masks/*')\nmask_df = pd.DataFrame()\nmask_df['file_names'] = all_mask_fn\nmask_df['mask_percentage'] = 0\nmask_df.set_index('file_names',inplace=True)\nfor fn in all_mask_fn:\n    mask_df.loc[fn,'mask_percentage'] = np.array(Image.open(fn)).sum()/(512*512*255) #255 is bcz img range is 255\n    \nmask_df.reset_index(inplace=True)\nsns.distplot(mask_df.mask_percentage)\nmask_df['labels'] = 0\nmask_df.loc[mask_df.mask_percentage>0,'labels'] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-test splitting of 85%-15%"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df,val_df = train_test_split(mask_df,test_size = 0.15,stratify = mask_df.labels,random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('No. of train files:', len(train_df))\nprint('No. of val files:', len(val_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_filepath = train_df['file_names'].tolist()\nval_filepath = val_df['file_names'].tolist()\ntrain_im_path = 'train'\ntrain_mask_path = 'masks'\nimg_size = 512","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding Augmentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"from albumentations import (\n    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n    ToFloat, ShiftScaleRotate,GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise,CenterCrop,\n    IAAAdditiveGaussianNoise,GaussNoise,OpticalDistortion,RandomSizedCrop\n)\ntrain_augment = Compose([\n    HorizontalFlip(p = 0.5),\n    ShiftScaleRotate(p = 0.5),\n    CLAHE(clip_limit = 2.0,tile_grid_size = (32,32)),\n   # ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03,p = 0.5),\n    RandomGamma(p = 0.5),\n    ToFloat()\n])\n\nAUGMENTATIONS_TRAIN = Compose([\n    HorizontalFlip(p=0.5),\n    OneOf([\n        RandomContrast(),\n        RandomGamma(),\n        RandomBrightness(),\n         ], p=0.3),\n    OneOf([\n        ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        GridDistortion(),\n        OpticalDistortion(distort_limit=2, shift_limit=0.5),\n        ], p=0.3),\n    RandomSizedCrop(min_max_height=(176, 256), height=512, width=512,p=0.25),\n    ToFloat()\n],p=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataGenerator Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    \n    def __init__(self,filepath = train_filepath,train_im_path = train_im_path,train_mask_path = train_mask_path,\n                 augmentations = None,img_size = img_size,batch_size = 64,nchannels = 3,shuffle = True):\n        \n        self.train_im_paths = list(filepath)\n        self.train_im_path = train_im_path\n        self.train_mask_path = train_mask_path\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.nchannels = nchannels\n        self.shuffle = shuffle\n        self.augmentations = augmentations\n        self.on_epoch_end()\n    \n    def __len__(self):\n        \n        return int(np.ceil(len(self.train_im_paths)/ self.batch_size))\n    \n    def __getitem__(self,index):\n        \n        indexes = self.indexes[index * self.batch_size : min((index + 1) * self.batch_size, len(self.train_im_paths))]\n        list_im_ids = [self.train_im_paths[i] for i in indexes]\n        X,y = self.data_generation(list_im_ids)\n        \n        if(self.augmentations is None):\n            return np.array(X,dtype = 'float32'),y\n        \n        im = []\n        for x in X:\n            augmented = self.augmentations(image = x)\n            im.append(augmented['image'])\n        return np.array(im,dtype = 'float32'),y\n    \n    def on_epoch_end(self):\n        \n        self.indexes = np.arange(len(self.train_im_paths))\n        if(self.shuffle):\n            np.random.shuffle(self.indexes)\n    \n    def data_generation(self,list_im_ids):\n        \n        X = np.empty((len(list_im_ids),self.img_size,self.img_size,self.nchannels))\n        y = np.empty((len(list_im_ids),))\n        for i,mask_path in enumerate(list_im_ids):\n            img_path = mask_path.replace(self.train_mask_path,self.train_im_path)\n            img = cv2.imread(img_path)\n\n            if(len(img.shape) == 2):\n                img = np.repeat(img[...,np.newaxis],3,2)\n            \n           # plt.imshow(img,cmap = 'bone')\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size))\n            y[i] = id_label_map[mask_path]\n\n        return np.uint8(X),y\n    \nid_label_map = {i : j for i,j in zip(mask_df.file_names.values,mask_df.labels.values)}\ndef chunker(sequence,size):\n    return (sequence[pos : pos + size] for pos in range(0,len(sequence),size))\ndef data_gen(list_files,id_label_map,batch_size,aug_func):\n    aug = aug_func\n    while True:\n        shuffle(list_files)\n        for block in chunker(list_files,batch_size):\n            x = [aug(image = cv2.imread(addr.replace('masks','train')))['image'] for addr in block]\n            y = [id_label_map[addr] for addr in block]\n            yield np.array(x),np.array(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing the Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = DataGenerator(batch_size=64,shuffle=False,augmentations=AUGMENTATIONS_TRAIN)\nimages,masks = a.__getitem__(0)\nmax_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im.squeeze(), cmap=\"bone\")\n    ax.axis('off')\nplt.suptitle(\"Chest X-rays, Red: Pneumothorax.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DenseNet169 model code"},{"metadata":{"trusted":true},"cell_type":"code","source":"def densenet_model(input_shape):\n    base_model = densenet.DenseNet169(include_top=False, weights=None, input_shape=input_shape)\n    x = base_model.output\n\n    out1 = GlobalMaxPooling2D()(x)\n    out2 = GlobalAveragePooling2D()(x)\n    #out3 = Flatten()(x)\n    out = concatenate([out1,out2])\n    out = BatchNormalization(epsilon = 1e-5)(out)\n    out = Dropout(0.4)(out)\n    fc = Dense(256,activation = 'relu')(out)\n    fc = BatchNormalization(epsilon = 1e-5)(fc)\n    fc = Dropout(0.3)(fc)\n    X = Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform', bias_initializer='zeros')(fc)\n    model =  Model(inputs=base_model.input, outputs=X)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = densenet_model((512,512,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stochatic Weight Averaging Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SWA(keras.callbacks.Callback):\n    \n    def __init__(self, filepath, swa_epoch):\n        super(SWA, self).__init__()\n        self.filepath = filepath\n        self.swa_epoch = swa_epoch \n    \n    def on_train_begin(self, logs=None):\n        self.nb_epoch = self.params['epochs']\n        print('Stochastic weight averaging selected for last {} epochs.'\n              .format(self.nb_epoch - self.swa_epoch))\n        \n    def on_epoch_end(self, epoch, logs=None):\n        \n        if epoch == self.swa_epoch:\n            self.swa_weights = self.model.get_weights()\n            \n        elif epoch > self.swa_epoch:    \n            for i in range(len(self.swa_weights)):\n                self.swa_weights[i] = (self.swa_weights[i] * \n                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n\n        else:\n            pass\n        \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.swa_weights)\n        print('Final model parameters set to stochastic weight average.')\n        self.model.save_weights(self.filepath)\n        print('Final stochastic averaged weights saved to file.')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cosine Annealing Learning Rate Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SnapshotCallbackBuilder:\n  \n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.01):\n      \n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix='Model'):\n\n        callback_list = [\n            callbacks.ModelCheckpoint(\"./best_model.h5\",monitor='val_loss', \n                                   mode = 'min', save_best_only=True, verbose=1),\n            swa,\n            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n        ]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner /= self.T // self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero / 2 * cos_out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compile the keras model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.compile(loss = sm.losses.bce_jaccard_loss,optimizer = SGD(learning_rate = 0.0001, momentum=0.0, nesterov=False),metrics = [sm.metrics.iou_score])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Code"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_filepath = train_df['file_names'].tolist()\nval_filepath = val_df['file_names'].tolist()\ntrain_im_path = 'train'\ntrain_mask_path = 'masks'\nepochs = 40\nsnapshot = SnapshotCallbackBuilder(nb_epochs = epochs,nb_snapshots = 1, init_lr = 1e-3)\nswa = SWA('./swa_weights.hdf5',37)\nbatch_size = 8\ntrain_generator = data_gen(train_filepath,id_label_map,batch_size,AUGMENTATIONS_TRAIN)\nval_generator = data_gen(val_filepath,id_label_map,batch_size,AUGMENTATIONS_TRAIN)\nhistory = model.fit_generator(train_generator,validation_data = val_generator,epochs = epochs,callbacks = snapshot.get_callbacks(),\n                              steps_per_epoch = (len(train_filepath) // batch_size) + 1,\n                              validation_steps = (len(val_filepath) // batch_size) + 1,\n                              )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting Model Accuracy against number of epochs"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='valid')\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"validation\"], loc=\"upper left\")\nplt.savefig('loss_performance.png')\nplt.clf()\nplt.plot(history.history['acc'], label='train')\nplt.plot(history.history['val_acc'], label='valid')\nplt.title(\"model acc\")\nplt.ylabel(\"acc\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"validation\"], loc=\"upper left\")\nplt.savefig('acc_performance.png')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}